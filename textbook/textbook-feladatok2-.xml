<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.0" xml:lang="hu">
    <info>
        <title>Helló, !</title>
        <keywordset>
            <keyword/>
        </keywordset>
    </info>
    <section>
        <title>FUTURE tevékenység editor</title>
        <para>
             Javítsunk valamit a ActivityEditor.java JavaFX programon! https://github.com/nbatfai/future/tree/master/cs/F6 Itt láthatjuk működésben az alapot: https://www.twitch.tv/videos/222879467
        </para>
        <para>
             A Future projekt a városok jövőbeli kutatására irányult a város lakóira alapozva, hogy azok innovatív és környezettudatos technológiai megoldásokat használjanak mindennapi feladatai megoldásában és hosszú távú céljai elérésében. Az F6-os projekt arra készült hogy a hallgatók a napi tevékeknységeiket feljegyezzék ugye környezettudatosan. 
             Az F6 projekt java programozési nyelven írt programja az ActivityEditor.java így ezt kellessz fordítanunk és futtatnunk, de ehhez szükségesek a többi fájlok is a projektből, illetve társítani kell a JavaFX-es könyvtárakat is. Mivel ebből elég sokat fog használni a programunk így egy szkriptet készítettem hozzá run néven melynek segítségével fordítom és futtatom is a programot:
        </para>
        <programlisting><![CDATA[
             #!/usr/bin/env bash
             export PATH_TO_FX="/usr/share/openjfx/lib"
             java_modules="javafx.controls,javafx.graphics,javafx.media,javafx.swing,javafx.web"
             rm -f *.class
             /usr/lib/jvm/java-11-openjdk-amd64/bin/javac\
                 --module-path $PATH_TO_FX\
                 --add-modules=$java_modules ActivityEditor.java
             /usr/lib/jvm/java-11-openjdk-amd64/bin/java\
                 --module-path $PATH_TO_FX\
                 --add-modules=$java_modules ActivityEditor
             rm -f *.class
        ]]></programlisting>
        <para>
             Az F6 projektet pedig az alábbi módon tölthetjük le, és indíthatjuk el miután megírtuk a run szkriptet:
        </para> 
        <programlisting><![CDATA[
             git clone https://github.com/nbatfai/future.git
             cd future/cs/F6
             gedit run
             ./run
        ]]></programlisting>
        <para>     
             A feladat az volt hogy javítsunk valamit a kódon, így kibővítettem egy törlés funkcióval amivel törölhetjük a mappáit vagy fájlait a megjelenített könyvtárszerkezetnek a gyökér mappa kivételével, mivel azt az ActivityEditor programunkban hozzuk létre a fa gyökereként nem fájlból olvassuk be. Ahogy az Én magam elemet is létrehozzuk faelemként nem beolvasással hozzuk létre, így ez a két elem fog látszani ha elindeítjuk a programot a City, gaming.props, initF6actFS.sh, me.props, programming.props fájlok használata nélkül. A program könyvtárszerkezete működik a  gaming.props, initF6actFS.sh, me.props, programming.props fájlok használata nélkül is. 
        </para>
        <programlisting language="Java"><![CDATA[
             javafx.scene.control.MenuItem deleteMenuItem = new javafx.scene.control.MenuItem("Törlés");
             addMenu.getItems().add(deleteMenuItem);
	     deleteMenuItem.setOnAction((javafx.event.ActionEvent evt) -> {
             javafx.scene.control.TreeItem item = getTreeItem();
             if (item.getParent() == null ){
             System.out.print("Cannot remove the root node.\n");}
             else{
             java.io.File file = getTreeItem().getValue();
             item.getParent().getChildren().remove(item);
	     deleteDirectory(file);}});}
             public boolean deleteDirectory(java.io.File directory) {
             if(directory.exists()){
             java.io.File[] files = directory.listFiles();
             if(null!=files){
             for(int i=0; i<files.length; i++) {
             if(files[i].isDirectory()) {
             deleteDirectory(files[i]);}
             else {
             files[i].delete();}}}}
             return(directory.delete());} 
        ]]></programlisting>
	<para>
	     Létrehoztunk Törlés menüpontot deleteMenuitem néven majd hozzáadtuk a Menu objektumhoz, majd beállítottunk hozzá egy akciót, mely egy megfelelő esemény bekövetkezésekor hívódik meg. A getTreeitem függvénnyel lekérjük a kijelölt mappát, majd ellenőrizzük almappa e, ha nem akkor hibaüzenetet írunk hogy gyökékönyvtárat nem lehet törölni, ha almappa akkor a remove függvénnyel töröljük az adatszerkezetből, majd töröljük a gépről is, ehhez viszont fájlként kell kezelnünk, majd meghívjuk rá az általunk készített deleteDirectory függvényt. Ez azért szüskséges mivel törölni szeretnénk a kijelölt mappánkat almappáival fájlaival együtt, azaz tartalmával együtt és a delete csak üres mappákkra vagy fájlokra hívható meg. A deleteDirectory függvény úgy működik hogy először ellenőrizzük a paraméterül kapott fájlról hogy létezik e, ha nem akkor a return részhez ugrunk ami meghívja a delete függvényt a nem létező fájlra és hibaüzenetet kapunk a delete függvénytől. Ha viszont létezik a fájl akkor az alfájlait belértve az almappákat is belerakjuk egy tömbbe, majd meghívjuk egyessével a tömb elemeit és ha mappa akkor meghívjuk rá rekurzívan a függvényünket egyébként töröljük a fájlt, majd a mappákat a rekurzív függvényhívások kiértékelődésekor törlil a függvények.                              	
	</para> 
             <mediaobject>
             <imageobject>
                 <imagedata fileref="future.png"></imagedata>
             </imageobject>
         </mediaobject>
    </section>	 

    <section>
        <title>OOCWC Boost ASIO hálózatkezelése</title>
        <para>
             Mutassunk rá a scanf szerepére és használatára! https://github.com/nbatfai/robocaremulator/blob/master/justine/rcemu/src/carlexer.ll
        </para>
        <para>
	     A sscanf függvény annyiban tér el a scanf változatától hogy ez az általunk meghatározott forrásból olvassa ki a szöveget, a scanf pedig a szabványos bemenetről. Mindkét függvény visszatérési értéke egy egész szám a parméterül adott referenciáknak a száma amikebe a szövegből beolvastunk értékeket, hiba esetén EOF karakterrel tér vissza, ami olyan karakter ami a char értékkészletén kívül esik. Mindkét függvény használatához az #include kisebbstdio.h> header használata szükséges, illetve az std:: névtér. A sscanf függvény első paramétere az hogy honnan próbálja meg ráilleszteni a második paraméterként megadott mintára a szöveget. A formátumot " " jelek között kell megadni, ahol akármilyen karaktereket megadhatunk akár whitespace-eket például szökózt is, illetve használhatunk karaktertípusokat is ekkor meg kell adni ugye a hozzátartozó változó referenciákat ahova mentse a beolvasott típus értékét illeszkedés esetén, ezeket a függvény további paramétereként adjuk meg. A karaktertípusoknak speciális jelölésük van például a char-nak c, string-nek s, decimal int d, f float point, ahol ugye a char az egy karaktert, a string szöveget, decimal int decimális azaz tízesszámrendszerbeli egész számot, a float point egy tizedes vesszőt tartalmazó nem valós számot. Sztring esetén egy white spac-ig olvas be karaktereket, ehhez char*, a char-hoz is, a float point-hoz float*, az int-hez int* referencia szükséges hogy tároljuk.   
	</para>
        <programlisting language="Java"><![CDATA[
	while ( std::sscanf ( data+nn, "<OK %d %u %u %u>%n", &idd, &f, &t, &s, &n ) == 4 ){
	nn += n;
	gangsters.push_back ( Gangster {idd, f, t, s} );}	
        ]]></programlisting>
        <para>
	     Ebben a példában a sccanf függvényt arra használjuk hogy a data sztring kisebbOK %d %u %u %u> alakú szövegrészeiből Gangster objektumokat hozzunk létre melyeket a gangsters vektorba tárolva különítünk el egymástól, mivel a vektor elemeit lekérdezhetjük majd. Ehhez ugye override-oltuk azaz túlterheltük a scanf függvényt, az nn változót valószínüleg 0 kezdőértékkel definiáltuk egész típussal, melynek értékét az n egész változó értékével növeljük minden beolvasott Gangster objektum után. Az n változó értékét minden beolvasás után a beolvasott sztring karakterjeinek számára módosítja a függvény, beleszámolja a nem illeszkedüket is, ezt a %n speciális karakter adja meg. Fontos hogy ez nem ugyanaz mint a függvény visszatérési értéke mert az fix szám és azoknak a paraméterül adott referenciáknak a száma amikbe beolvasunk a bemenetből. A visszatérési értéke jelen esetben 4, ha jól működik a függvényünk, azaz betud olvasni egy Gangster objektum létrehozásához elegendő változókat, azaz a konstruktorának változóit. A while ciklus addig fogja újból meghívni a scanf függvényt ameddig tud beolvasni újabb gangsters vektor elemet azaz Gangster objektumot. Ahhoz hogy ne olvassuk be kétszer ugyanazt a Gangster objektumot ahhoz növeljük a beolvasási pointer értékét a már beolvasott sztringek hosszával. A %u az unsigned decimal int-eket jelenti azaz aláírás nélküli decimélis egész számokat. Ennek a decimális szava a számjegyeire utal ami tizes számrendszerbeli az 0,1,2,3,4,5,6,7,8,9 számjegyekből épül fel. Az aláírás nélküli rész pedig arra hogy nem tartalmaz negatív egészeket, ehelyett dupla akkora pozitív egészeket tartalmaz 0-val kezdődően mint a sima int típus.           
	</para>
    </section>
		
    <section>
        <title>SamuCam</title>
        <para>
	     Mutassunk rá a webcam (pl. Androidos mobilod) kezelésére ebben a projektben: https://github.com/nbatfai/SamuCam
	</para>
        <para>            
             A SamuCam projekt segítségével a számítógépünk kamerája felismeri az arcunkat, ehhez azonban szükségünk van egy kamerára, ha viszont nincs akkor az Androidos telefonunkra telepítenünk kell az IP WEBCAM applikációt, melyben a Start server opcióval elindíthatjuk a vidó streamet a telefon kameráján keresztül, ez megad nekünk egy ip címet amit a programunk futtatásakor az --ip ipcím kapcsolóval kell csatolni. Ahhoz hogy a gépen tudjuk futtatni az alábbiakat kell tennünk:
</para>
	     <programlisting><![CDATA[
		sudo apt-get install libopencv-dev qt5-default
		git clone https://github.com/nbatfai/SamuCam.git
		cd SamuCam/
		wget https://github.com/Itseez/opencv/raw/master/data/lbpcascades/lbpcascade_frontalface.xml
		qmake SamuLife.pro
 		make
 		./SamuCam
	     ]]></programlisting>
<para>	
     A libopencv-dev csomag az OpenCV osztályok használatához szüskéges mivel ezekkel dolgozzuk fel a kamera által streamelt adatokat, illetve ezzel végezzük el az arcfelismerést. A QT keretrendszert is telepítjük mivel használunk a programban például a QT slot-sigal mehacnizmusát, illetve ezzel fogjuk fordítani a projektet a hozzátartozó qmake paranccsal. A git clone paranccsal pedig letöltjük magát a SamuCam projektet és a wget parancsal pedig letöltünk egy xml fájlt ami az arcfelismerő működéséhez szükséges adatokat tartalmazza.
</para>	     
<programlisting language="C++"><![CDATA[
		SamuCam::SamuCam ( std::string videoStream, int width = 176, int height = 144 )
		  : videoStream ( videoStream ), width ( width ), height ( height ){
		  openVideoStream();}
		SamuCam::~SamuCam (){}
		void SamuCam::openVideoStream(){
  		videoCapture.open ( 0 );
  		videoCapture.set ( CV_CAP_PROP_FRAME_WIDTH, width );
  		videoCapture.set ( CV_CAP_PROP_FRAME_HEIGHT, height );
  		videoCapture.set ( CV_CAP_PROP_FPS, 10 );}
	     ]]></programlisting>
<para>	
     A feladat alapján a webcam működéséről kell írnunk ezt pedig a SamuCam.cpp fájl tartalmazza. A program elején definiáltuk a SamuCam konstruktorát, melynek elsőparamétere az általunk megadott ip cím lesz de ezt ugye a main függvénybe definiáltuk ahol kezeljük a parancssori argumentumokat egy SamuCam konstruktor meghívással. Ezt az értéket mentjük egy videoStream sztring típusú változóba, a második és harmadik paraméter értékét megadtuk. Ezen értékekekkel inicializáljuk az aktuális objektumunk változóit, majd meghívjuk az openVideoStream függvényt. Ennek a függvénynek nincs visszatérési értéke sem paramétere, de ősosztálya a SamuCam osztálynak így örökli az aktuális objektum változóinak értékét, a videoCapture.open függvénnyel nyitjuk meg a videósztreamet, majd beáll1tjuk a set függvénnyel a stream képernyő méretét, illetve a maximum fps értékét. Ezek a beáll1tások azért szükségesek mert a programunk eléggé erőforrásigényes és ezzel próbáljuk meg elérni hogy lefusson lefagyás nélkül. Az open függvény paramétere lehetne 0 is ekkor a laptopunk webkameráját használná a program, illetve ip cím helyett megadhatnánk már létező videót is. A run függvényben hajtjuk végre az arcfelismerést, ez örökli az aktuális SamuCam objektum tulajdonságait, mivel felhasználjuk a videoCapture objektumát, illetve a SamuCam örökli a QThread osztályt mivel a run függvényt az tartalmazza. A run függvény indítja el a szálak működését, de jelen esetben overridoltuk, mivel példányosítottunk benne egy CascadeClassifier objektumot, mivel ez az osztály felelős azért hogy felismerjünk egy objektumot a videósztreamből. Ehhez az oszályhoz tartoznak a load és detectMultiScale függvények. A load függvény segítségével töltjük be a detektáláshoz szükséges xml fájlokat, jelen esetben az arcfelismeréshez szükséges fájlt, a betöltést hibakezeltük egy if feltétellel. A detectMultiScale függvény végzi az objektumok detektálását, melynek első paramétere egy Mat objektum referenciája, ezen fogja végezni a detektálást, a második paraméter kisebbRect> típusú vektor referenciája, ebbe fogja tárolni a detektált objektumokat, a harmadik paraméterben a skálafaktor, ami azt adja meg milyen mértékben csökkenjen a képméret skálázásonként, a harmadik paraméter azt adja meg minimum hány szomszádos téglalapnak kell lennie az adott téglalapnak, minSize a minimális objektum méret, maxSize a maimális objektum méret. Tehát mivel a felismerő függvény bemenetje egy Mat típusú objektum így létrehozunk egyet, és beleolvassuk a videósztreamet a read függvény segítségével. A read függvényt egy while ciklusba rakjuk hgoy addig olvasson amíg van bemenet, ezt pedig egy másik while ciklusba hogy addig olvasson amíg meg van nyitva a videósztream. A beolvasás után ellenőrizzük hogy üres e beolvasott Mat objektum, ha nem akkor a resize függvénnyel visszaméretezzük a már beolvasott képet az eredeti méretre, majd a cvtColor függvénnyel módosítjuk a beolvasott kép színét fekete-fehérre, ez is arc felismeréshez szükséges, mivel fekete-fehér képre van az xml forrásfájl. A fejete-fehér kép egy új Mat objektumba kerül, ezt az equalizeHist függvénnyel javítunk a képminőségen. Majd létrehozunk egy kisebbrectangle> elemekből álló vektort amibe mentjük a detektált objektumokat a detectMultiScale függvény. Ha sikeres volt az arcfelismerés tehát a vektorunk elemszáma nagyobb 0 akkor készítünk egy másik Mat objektumot a vektorunk 0-ik elemének másolatával, amit a clone függvénnyel másolunk le, illetve ezzel allokálunk neki memóriát is, hogy a többi objektum ne módosuljon. Ebből az új objektumból létrehozunk egy új képet. Majd létrehozunk két pontot a Point objektumok létrehozásával, melyre illesztünk egy téglalpot a rectangle függvénnyel a Scalar függvényt felhasználva halványkék színnel a Mat objektumra ahova beolvastuk a videósztreamet részletet. Majd meghívjuk az emit függvénnyel a slot-signal mehanizmust, majd létrehozunk egy újabb képet az objektumból ahova beolvastuk a videósztream részletet és ismét meghívunk egy emit signal-slot mehacnizmust. A sleep függvényekkel késleltetjük a szálat a paraméterül adott miliszekundomokig, ezt valószínüleg a hatékonyabb működés érdekében használjuk. A signal-slot mehacnizmus lehet felelős azért hogy kommunikáljon másik objektummal hogy jelenítse meg a képeket. Az if feltétellel azt ellenőrizzük hogy megvan e még nyitva a videósztream, ha nincs újra megnyitjuk.                               
	</para>    
             <mediaobject>
             <imageobject>
                 <imagedata fileref="samucam.png"></imagedata>
             </imageobject>
         </mediaobject>
    </section>	 

    <section>
        <title>BrainB</title>
        <para>
	     Mutassuk be a Qt slot-signal mechanizmust ebben a projektben: https://gi thub.com/nbatfai/esporttalent-search
	</para>
        <para>            
             A BrainB projekt a játékbeli karakterelvesztést szimulálja, mivel a Samu Entropy négyzet közepén lévő fekete ponton kell tartani az egeret, ha ez sikerül akkor elkezd mozogni és megsokszorozódni a négyzetek száma. Ha viszon elvesztjük Samu Entropy-t azaz nem tudjuk a fekete pontján tartani a kurzort akkor csökkeni fog a négyzetek száma és lassul a mozgása is. A teszt 10 percig tart, erről kapunk információt az ablak felső sorában, az eredméyeinket a README.md fájl tartalmazza. Ez lost/found részekből áll a karakter elvesztése és megtalálásáról, én kb. két perc után kiléptem, így meghívódott a Close Event melyben definiáltuk hogy hívja meg a save függvényt ami menti az teszt adatait és leállítja a programot beépített signal-slot mechanizmus segítségével Test néven bővítve az eeltelt milliszekundumok számával. A projekt működéséhez szüskégesek az alábbi lépések végrehajtása a parancssorban:
</para>	
     <programlisting><![CDATA[
		sudo apt-get install libopencv-dev qt5-default
		git clone https://github.com/nbatfai/esport-talent-search.git
		cd esport-talent-search
		qmake BrainB.pro
 		make
 		./BrainB
	     ]]></programlisting>		
<para>       
      A program használatához szükségesek az opencv és qt kiegészítőket telepíteni, a QT fájlokból a qmake paranccsal létrehozzuk a MakeFile-t amiből a make paranccsal létrehozzuk a futtatható állományokat. A QT keretkrendszer segítségével tudjuk használni a signal-slot mechanizmust ami az objektumok közötti kommunikációt valósítja meg. A signal-ok függvények visszatérési értél és definícó nélkül, azonban rendelkeznek deklarációval és paraméterekkel. Illetve a signálokat nem a hagyományos módon lehet meghívni, hanem az emit kulcsszó seg1tségével. A signal függvények meghívódása esetén automatikusan meghívódinak a hozzákapcsolt slot-ok.  A slot-okat már meghívhatjuk a hagyományos módon, lehet definíciójuk, paraméterük, de visszatérési értékük nem. A signal és slot-ok összekapcsolását a connect függvény véggzi, ehhez azonban az szükséges hogy a signalok paramétere megeggyezzen a slot-ével vagy a slot-nak ne legyen paramétere mivel a szignal átadhatja neki. A connect függvényeket kéféleképpen használhatjuk megadhatjuk a SIGNAL() SLOT() kifejezéseket, de el is hagyhatjuk belőle ekkor azonban andosztálynév:: -tal kell helyettesíteni őket. Mivel az utóbbi eset a frissebb verzió így célszerű úgy használni a connect függvényeket. A connect függvény első paramétere az objektum ami a szignált küldi, a második paraméter a szignál amit meghívhatunk majd, a harmadik paraméter a slot objektuma, a negyedik paraméter a slot amit meghív a signal. A QT osztályok mindegyike tartalmaz beépített szignál-okat és slot-okat, ezeket akár felül is definiálhatjuk. Ha viszont saját osztályokat használunk akkor leszármazottja kell hogy legyen a QObject osztálynak és tartalmaznia kell a Q_OBJECT makrót. Ez a QT osztályokra nem érvényes mivel azok leszármazottjai a QObject osztálynak. Fontos megjegyezni hogyha túl sok szignál-slot kifejezéseket tartalmaz a programunk akkor jóval lassabban fog működni mint azok nélkül. 
</para>	
     <programlisting language="c++"><![CDATA[
	       connect ( brainBThread, SIGNAL ( heroesChanged ( QImage, int, int ) ),
    	       this, SLOT ( updateHeroes ( QImage, int, int ) ) );

	       connect ( brainBThread, SIGNAL ( endAndStats ( int ) ),
    	       this, SLOT ( endAndStats ( int ) ) );
	     ]]></programlisting>		
<para>       
      Az első esetben brainBThread objektum heroesChanged szignál-ja Samu Entropy objektumunk pozíciójának aktuális koordinátáit illetve az abalkunknak az aktuális képét adjuk tovább az aktuális objektum updateHeroes függvényének ami ugye a BrainBWin objktum része. Az updateHeroes függvény a kapott értékekből kiszámítja a MouseEvent értékeinek segítségével azaz az egérpozíciókból hogy elvesztettük e a karakterünket azaz Samu Entropy-t vagy nem, illetve ha 12-nél többször bekövetkezik egymás után egy eset akkor meghívunk rá egy függvényt. Ez esetenként eltérő, karakterelvesztéskor törlünk egy New Entropy-t azaz a többi karakter közül egyet, illetve csökkentünk a mozgási sebesség értékén, de ezek itt még nem valósulnak meg a képernyőn. A másik esetben kilépünk a progrmaból ha 300-nál több New Samu objektumunk van, illetve mindkét esetben végzünk statisztikai mentéseket is. Majd pixmap-pá alakítjuk az ugye paramétertőlkapott képünket és meghívjuk az update() függvényt mellyel Paint Event-et hívunk meg és frissítjük az ablakot.
	     A második esetben az emit függvény a run függvényben van azaz az endAndStats függvény ott hívódik meg. A run függvény egy while ciklussal kezdődik  ami addig fut amíg le nem tellik a 10 perc, ezt minden ciklus után ellenőrzi, ha letellik akkor meghívódik az endAndStat szignál ami kiírja a szabványos kimenetre hogy köszönjük a program használatát és hogy hova mentette a tesztünk eredményét, ezt a mentést el is végzi és leállítja a program működését a save függvénnyel és a Close Event-et meghívó close függvénnyel. A run függvény ciklusában meghívjuk a devel függvényt, mely a move függvény és egy kondíciós feltétel segítségével a vektor objektumainak kisebbik elemét elosztja tízzel, ezzel módosíthatjuk a téglalpok elhelyezkedését. Majd még meghívódik a draw függvény amely a kirajzolást végzi, elkészíti az ablak alakzatait, majd meghívja az emit függvénnyel az előbbi herosChanged szignált.
	</para>
             <mediaobject>
             <imageobject>
                 <imagedata fileref="brainb.png"></imagedata>
             </imageobject>
         </mediaobject>
    </section>	 

    <section>
        <title>GPS Tracker</title>
        <para>
	     Alternatívaként készíthetsz egy GoogleMaps alapú Androidos „GPS trackert”, 2007 óta csinálok ilyen példát: https://youtu.be/QStgBZ6JfAU az aktuális a Bátfai Haxor Stream keretében: https://bhaxor.blog.hu/-2018/09/19/nandigps_ismerkedes_a_gps-el
	</para> 
        <para>
	     Mivel Andoid alapú programot fogunk használni így ennek létrehozásához szükségünk van egy androidos programozó 0felületre, erre biztosít lehetőséget az Anroid Studio, melyben Google Maps projektet hozhatunk létre java programozási nyelven, ehhez az alábbi linkről kell letöltenünk az Android Studio alkalmazást: https://developer.android.com/studio/?gclid=EAIaIQobChMI56_f0a3t5QIVyYGyCh0arw8kEAAYASAAEgILFPD_BwE ,majd a parancssorban az alábbi műveletet kell végrehajtanunk:
</para>	
     <programlisting><![CDATA[
		cd ~/Letöltések
		tar xzvf android-studio-ide-191.5977832-linux.tar.gz
 		cd android-studio/bin
 		./studio.sh
	     ]]></programlisting>		
<para>	
     A program elindulása után feltelepítjük az alapértelmezett beállításokkal, majd kiválasztjuk az új projekt létrehozását, azon belül pedig a Google Maps típust ahol beállítjuk a java-t programozási nyelvnek. A projekt betöltődése után láthatjuk agoogle_maps_api.xml fájl forráskódját, mely arra szolgál hogy lekérjünk egy API kódot a forráskódban megadott linkről és megadjuk a forráskód YOUR_KEY_HERE helyre. Az API KEY-re azért van szükségünk hogy kapcsolatba léphessünk a Google Maps szerverekkel. A másik fájlunk a MapsAcitivity.java melyben láthatunk egy java programozási nyelven írt kódot, ezt futtatva láthatjuk hogy megnyitja a Google térképet és bejelöli Sydny városát. A futtatáshoz szüskégünk van egy Android-os eszközre melyre telepítettük az OEM USB Driver-t az alábbi linkről: https://developer.android.com/studio/run/oem-usb#Drivers. Amint láthatjuk a forráskód onMapReady függvényében hozzuk létre sydney objektumot a hozzátartozó szélességi és hosszúsági koordinátáival megadva, majd bejelöljük a térképen Sydney városát létrehozva egy markert a koordinátáiból és egy általunk adott névvel, majd a kamerát Sydney-re állítjuk a moveCamera függvény segítségével. Mivel mi a saját pozíciónkat akarjuk meghatározni a térképen és ezt nyomonkövetni ehhez haználnunk kellessz a LocationManager és LocationListener osztályokat, melyek segítségével lekérhetjük az aktuális tartózkodási pozíciónkat. Az onResume egy virtuális függvény mely a program induálaskor hívódik meg és hívja meg a getLocation() függvényt, melyben példányosítunk egy LocationManager objektumot melyben megadjuk a helyazonosításhoz használt szolgáltatás nevét, majd a létrehozott objektum isProviderEnabled függvényének segítségével ellneőrizzük hogy elérhető e az adott provider ami a NETWORK vagy a GPS, majd meghívjuk az objektumunk requestLocationUpdates függvényét amely frissíti a helyadatokat a NETWORK_PROVIDER vagy GPS_PROVIDER segítségével, melyet az első paramétereként adunk meg, második paraméterként hogy hány miliszekudnumonként, harmadik paraméterben hogy hány méterenként, a negyedik paraméterként a megfelelő LocationListener objektumot mely meghívja az onLocationChanged() függvényét, melyet ha először hívunk meg akkor a Sydney-s marker-hez hasonlóan elkészítjük a markerünket a térképen, de itt most a Location objektum szélességi és hosszúsági koordinátáit használjuk fel, melyet a NETWORK vagy GPS határoz meg, a helység nevét pedig ahol vagyunk a getLocationName függvény határoz meg az aktuális Location objektumra meghívva. Amint láthatjuk a getLocationName függvény visszatérési értéke egy Sztring lesz, ugye az adott helyiség neve. A függvényben a Geocoder objektum segítségével a meghívott Location objektum szélességi és hosszúsági koordinátáiból a getFromLocation függvény határoz meg egy címet, melyet a harmadik paraméteréen állítottunk be. Ezt a címet elmentjük egy List objektumba melynek ellenőrizzük hogy a mérete nagyobb e mint 0, ha igen akkor getLocality() függvénnyel visszaadjuk a település nevét. Ha nem talált címet akkor az error sztringet adjuk vissza. Majd a firstCall értékétfalse-ra állítjuk, ezzel jelezzük hogy a következő híváskor ami 5 milliszekundum vagy 1 méter múlva lesz az else ág hívódjon meg ahol az addPolyline függvény segítségével a marker akutális pozíciójától, a jelenlegi helyzetünkig rajzolunk egy kék 5 pixel vastagságú vonalat, majd a remove függvénnyel töröljük a markert és újat adunk hozzá az eddigiekhez hasonlóan. Majd az onLocationChanged függvény végén a kamerát rázoomoljuk ahova a marker-t raktuk az aktuális Location objektumunkra 17x-eres zoom értékre. Fontos még hogy mivel helymeghatározást használunk, ezért szükséges a felhasználó engedélye hozzá hogy meghatározzuk a helyét, ezt a CheckPermission függvénnyel ellenőrizzük, melyben ha nem kaptunk engedélyt a felhasználótól, akkor kérünk egy if feltétel segítségével. Ezt az onCreate függvényben hívtuk meg, ami a program futtatásakor lefut egyszer, az onResume függvény is lefut ekkor, de ez többször is lefuthat miután folytatódik az alkalmazás működése szünetelés után. A szünetelést az onPause függvénnyel kezelejük, ekkor ha belekezdtünk a helymeghatározásba akkor töröljük a frissítéseit a LocationListener függvénynek, hogy majd az onResume gond nélkül meghívhassa újra az eljárást. A program szünetel ha háttérben futtatjuk, azaz mást csinálunk a telefonon példul telefonálunk, illetve ha elfelejtenénk bezárni ne merítse le fölöslegesen a telefonunkat.
</para>	
     <programlisting language="java"><![CDATA[
		package com.example.myapplication;
		
		import androidx.core.app.ActivityCompat;
		import androidx.core.content.ContextCompat;
		import androidx.fragment.app.FragmentActivity;

		import android.Manifest;
		import android.content.Context;
		import android.content.pm.PackageManager;
		import android.graphics.Color;
		import android.location.Address;
		import android.location.Geocoder;
		import android.location.Location;
		import android.location.LocationListener;
		import android.location.LocationManager;
		import android.os.Bundle;
		import android.widget.TextView;
		import android.widget.Toast;

		import com.google.android.gms.maps.CameraUpdateFactory;
		import com.google.android.gms.maps.GoogleMap;
		import com.google.android.gms.maps.OnMapReadyCallback;
		import com.google.android.gms.maps.SupportMapFragment;
		import com.google.android.gms.maps.model.LatLng;
		import com.google.android.gms.maps.model.LatLngBounds;
		import com.google.android.gms.maps.model.Marker;
		import com.google.android.gms.maps.model.MarkerOptions;
		import com.google.android.gms.maps.model.PolylineOptions;

		import java.io.FileNotFoundException;
		import java.util.List;
		import java.util.Locale;

		public class MapsActivity extends FragmentActivity implements OnMapReadyCallback {

    		private GoogleMap mMap;
    		LocationManager mlocationManager;
    		LocationListener listenerGPS;
    		LocationListener listenerNetwork;
    		private boolean firstCall = true;
    		private boolean gps_enabled;
    		private boolean network_enabled;
    		private Marker marker;

    		@Override
    		protected void onCreate(Bundle savedInstanceState) {
        		super.onCreate(savedInstanceState);
        		setContentView(R.layout.activity_maps);
        		// Obtain the SupportMapFragment and get notified when the map is ready to be used.
        		SupportMapFragment mapFragment = (SupportMapFragment) getSupportFragmentManager()
                		.findFragmentById(R.id.map);
        		mapFragment.getMapAsync(this);
        		CheckPermission();

        		listenerGPS = new LocationListener() {
		
            		@Override
            		public void onLocationChanged(@org.jetbrains.annotations.NotNull Location location) {
                		LatLng mylocation = new LatLng(location.getLatitude(), location.getLongitude());
                		if(firstCall){
                    		marker = mMap.addMarker(new MarkerOptions().position(mylocation).title("You are in " + getLocationName(location)));
                    		marker.showInfoWindow();
                    		firstCall = false;
                		}else {
                    		mMap.addPolyline(new PolylineOptions()
                            		.add(marker.getPosition(), mylocation)
                            		.width(5)
                            		.color(Color.BLUE));
                    		marker.remove();
                    		marker = mMap.addMarker(new MarkerOptions().position(mylocation).title("You are in " + getLocationName(location)));
                    		marker.showInfoWindow();
                		}
                		mMap.moveCamera(CameraUpdateFactory.newLatLngZoom(mylocation, 17f));
            		}
		
            		@Override
            		public void onStatusChanged(String provider, int status, Bundle extras) {
            		}

            		@Override
            		public void onProviderEnabled(String provider) {
            		}

            		@Override
            		public void onProviderDisabled(String provider) {
            		}
        		};

        		listenerNetwork = new LocationListener() {

            		@Override
            		public void onLocationChanged(@org.jetbrains.annotations.NotNull Location location) {
                		LatLng mylocation = new LatLng(location.getLatitude(), location.getLongitude());
                		if(firstCall){
                    		marker = mMap.addMarker(new MarkerOptions().position(mylocation).title("You are in " + getLocationName(location)));
                    		marker.showInfoWindow();
                    		firstCall = false;
                		}else {
                    		mMap.addPolyline(new PolylineOptions()
                            		.add(marker.getPosition(), mylocation)
                            		.width(5)
                            		.color(Color.BLUE));
                    		marker.remove();
                    		marker = mMap.addMarker(new MarkerOptions().position(mylocation).title("You are in " + getLocationName(location)));
                    		marker.showInfoWindow();
                		}
                		mMap.moveCamera(CameraUpdateFactory.newLatLngZoom(mylocation, 17f));
            		}

            		@Override
            		public void onStatusChanged(String provider, int status, Bundle extras) {
            		}
		
            		@Override
            		public void onProviderEnabled(String provider) {
            		}

            		@Override
            		public void onProviderDisabled(String provider) {
            		}
        		};
    		}

    		@Override
    		public void onResume() {
        		super.onResume();
        		getLocation();
    		}

    		protected void onPause() {
        		super.onPause();
        		if(gps_enabled) mlocationManager.removeUpdates(listenerGPS);
        		if(network_enabled) mlocationManager.removeUpdates(listenerNetwork);
    		}

    		@Override
    		public void onMapReady(GoogleMap googleMap) {
        		mMap = googleMap;
    		}

    		public void CheckPermission() {
        		if (ContextCompat.checkSelfPermission(getApplicationContext(), android.Manifest.permission.ACCESS_FINE_LOCATION) != PackageManager.PERMISSION_GRANTED &&
                		ActivityCompat.checkSelfPermission(getApplicationContext(), android.Manifest.permission.ACCESS_COARSE_LOCATION) != PackageManager.PERMISSION_GRANTED){
           		ActivityCompat.requestPermissions(this, new String[]{android.Manifest.permission.ACCESS_FINE_LOCATION, android.Manifest.permission.ACCESS_COARSE_LOCATION}, 101);
        		}
    		}

    		public void getLocation() {
        		try {
            		mlocationManager = (LocationManager) getSystemService(Context.LOCATION_SERVICE);
            		if(network_enabled = mlocationManager.isProviderEnabled(LocationManager.NETWORK_PROVIDER)) {
                		mlocationManager.requestLocationUpdates(LocationManager.NETWORK_PROVIDER, 50, 1f, listenerNetwork);
            		}
           		if(gps_enabled = mlocationManager.isProviderEnabled((LocationManager.GPS_PROVIDER))) {
                		mlocationManager.requestLocationUpdates(LocationManager.GPS_PROVIDER, 50, 1f, listenerGPS);
            		}
        		} catch (SecurityException e) {
            		e.printStackTrace();
        		}
    		}

    		public String getLocationName(Location location){
        		Geocoder gcd = new Geocoder(this, Locale.getDefault());
        		try {
            		List<Address> addresses = gcd.getFromLocation(location.getLatitude(), location.getLongitude(), 1);
            		if (addresses.size() > 0) {
                		return addresses.get(0).getLocality();
            		}else return "error";
        		}catch(Exception e) {
            		throw new RuntimeException(e);
        			}
    			}	
		}
	     ]]></programlisting>		
<para>	
Használhatjuk az applikációt Ubuntu alatt is úgy hogy rámegyünk az Anroid Stuido-ba a Run/Select Device.../Open AVD Manager ahol alpértelmezett beállításokkal létrehozhatunk egy virtuális telefont, ehhez azonban a CPU-nknak támogatnia kell az SVM és VTX-et. 
	</para>
             <mediaobject>
             <imageobject>
                 <imagedata fileref="gpstracker.png"></imagedata>
             </imageobject>
         </mediaobject>
    </section>	    
</chapter>		
